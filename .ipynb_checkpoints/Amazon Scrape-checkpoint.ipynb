{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fake-useragent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import dateparser\n",
    "from fake_useragent import UserAgent\n",
    "ua = UserAgent()\n",
    "from lxml import html\n",
    "import time\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Product Details From Grid Menu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The product scraper will work on search result pages, and will take every product that appears in the grid, and keep going to the next page until there are no more pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_products(page, prod_dict):\n",
    "    \n",
    "    #XPATHS\n",
    "    #xpath for amazon product list item\n",
    "    xpath_product = \"//li[starts-with(@id, 'result')]\"\n",
    "    #xpath for next product arrow link\n",
    "    xpath_next_page = \"//a[@id='pagnNextLink']/@href\"\n",
    "\n",
    "    #set up the request\n",
    "    headers = {'User-Agent': ua.random}\n",
    "    r = requests.get(page, headers=headers)\n",
    "    if r.status_code != 200:\n",
    "        print('status error')\n",
    "\n",
    "    #get test response from request\n",
    "    products_page = r.text\n",
    "    \n",
    "    #parse the page\n",
    "    parser = html.fromstring(products_page)\n",
    "    \n",
    "    \n",
    "    # get the individual products  \n",
    "    products = parser.xpath(xpath_product)\n",
    "    \n",
    "    \n",
    "    #iterate through products and get their info\n",
    "    for product in products:\n",
    "        \n",
    "        # take the amazon asin unique identifier\n",
    "        try:\n",
    "            prod_dict['asins'].append(product.xpath(\"@data-asin\")[0])\n",
    "        except:\n",
    "            prod_dict['asins'].append(\"none\")\n",
    "            \n",
    "            \n",
    "        # take the product title    \n",
    "        try:\n",
    "            prod_dict['titles'].append(\n",
    "                product.xpath(\"div/div[3 or 4]/div[1]/a/h2/text()\")[0])\n",
    "        except:\n",
    "            prod_dict['titles'].append(\"none\")\n",
    "            \n",
    "            \n",
    "        #take the number of reviews (this can be in 2 places based on product detail)\n",
    "        try:\n",
    "            # if 1st found element has 5 or more characters it is a 'more products' link, so take 2nd element\n",
    "            if len(\n",
    "                    product.xpath(\n",
    "                        \"div/div[6 or 7]/a[starts-with(@class, 'a-size-small')]/text()\"\n",
    "                    )[0]) > 5:\n",
    "                prod_dict['review_counts'].append(\n",
    "                    int(\n",
    "                        product.xpath(\n",
    "                            \"div/div[6 or 7]/a[starts-with(@class, 'a-size-small')]/text()\"\n",
    "                        )[1]))\n",
    "            else:\n",
    "                prod_dict['review_counts'].append(\n",
    "                    int(\n",
    "                        product.xpath(\n",
    "                            \"div/div[6 or 7]/a[starts-with(@class, 'a-size-small')]/text()\"\n",
    "                        )[0]))\n",
    "        except:\n",
    "            prod_dict['review_counts'].append(0)\n",
    "            \n",
    "            \n",
    "        # take the price of the product (can appear in 1 of 2 places)    \n",
    "        try:\n",
    "            if len(product.xpath(\"div/div[5 or 7]/div[1]/a/span/text()\")[\n",
    "                    0]) == 0:\n",
    "                prod_dict['prices'].append(\n",
    "                    (\n",
    "                        product.xpath(\"div/div[5 or 7]/div[1]/a/span/text()\")[\n",
    "                            1].replace(\"£\", \"\").replace(\",\", \"\")))\n",
    "            else:\n",
    "                prod_dict['prices'].append(\n",
    "                    (\n",
    "                        product.xpath(\"div/div[5 or 7]/div[1]/a/span/text()\")[\n",
    "                            0].replace(\"£\", \"\").replace(\",\", \"\")))\n",
    "        except:\n",
    "            prod_dict['prices'].append(np.nan)\n",
    "            \n",
    "            \n",
    "    # get the next page from the arrow link\n",
    "    next_page = parser.xpath(xpath_next_page)\n",
    "    \n",
    "    #if no more pages, return the dictionary\n",
    "    if len(next_page) == 0:\n",
    "        return prod_dict\n",
    "    \n",
    "    #otherwise go to next page and repeat \n",
    "    else:\n",
    "        page = \"https://www.amazon.co.uk\" + next_page[0]\n",
    "        return get_products(page, prod_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling the above function - go grab a quick coffee whilst this runs.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.amazon.co.uk/s/ref=nb_sb_noss_2?url=search-alias%3Daps&field-keywords=playmobil\"\n",
    "\n",
    "prod_dict = {'asins':[],\n",
    "            'titles':[],\n",
    "            'review_counts':[],\n",
    "            'prices':[]}\n",
    "\n",
    "playmobil_products = get_products(url,prod_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_play = pd.DataFrame(playmobil_products,columns=prod_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the products to .CSV\n",
    "df_play.to_csv('amazon_playmobil.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Reviews from Review Pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions for picking out the salient details from a review block\n",
    "\n",
    "def get_asin(review):\n",
    "    xpath_asin = \".//a[@data-hook='review-title']/@href\"\n",
    "    return review.xpath(xpath_asin)[0][-10:]\n",
    "\n",
    "def get_review_id(review):\n",
    "    return review.xpath(\"@id\")[0]\n",
    "\n",
    "\n",
    "def get_stars(review):\n",
    "    xpath_stars = \".//i[@data-hook='review-star-rating']//text()\"\n",
    "    return review.xpath(xpath_stars)[0][0]\n",
    "\n",
    "\n",
    "def get_title(review):\n",
    "    xpath_title = \".//a[@data-hook='review-title']//text()\"\n",
    "    return review.xpath(xpath_title)[0]\n",
    "\n",
    "\n",
    "def get_comment(review):\n",
    "    xpath_comment = \".//span[@data-hook='review-body']//text()\"\n",
    "    if review.xpath(xpath_comment) != []:\n",
    "        return review.xpath(xpath_comment)[0]\n",
    "    else: \n",
    "        return \"QQQQQQQQQ\" \n",
    "\n",
    "\n",
    "def get_author(review):\n",
    "    xpath_author = \".//a[@data-hook='review-author']/@href\"\n",
    "    if review.xpath(xpath_author) != [] and len(review.xpath(xpath_author)[0]) > 26:\n",
    "        return review.xpath(xpath_author)[0][26:]\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def get_date(review):\n",
    "    xpath_date = \".//span[@data-hook='review-date']//text()\"\n",
    "    return review.xpath(xpath_date)[0][3:]\n",
    "\n",
    "\n",
    "def get_verified(review):\n",
    "    xpath_verified = \".//span[@data-hook='avp-badge']//text()\"\n",
    "    if review.xpath(xpath_verified) != []:\n",
    "        return review.xpath(xpath_verified)[0]\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def get_helpful_count(review):\n",
    "    xpath_helpful = \".//span[@data-hook='helpful-vote-statement']//text()\"\n",
    "    if review.xpath(xpath_helpful) != []:\n",
    "        score = review.xpath(xpath_helpful)[0].split()[0]\n",
    "        if score == \"One\":\n",
    "            return 1\n",
    "        else:\n",
    "            return score\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def get_image_count(review):\n",
    "    xpath_image = \".//img[@data-hook='review-image-tile']\"\n",
    "    if review.xpath(xpath_image) != []:\n",
    "        return len(review.xpath(xpath_image))\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def get_author_status(review):\n",
    "    xpath_status = \".//span[@data-hook='review-author']/following-sibling::span[@class='a-size-mini a-color-link c7yBadgeAUI c7yTopDownDashedStrike c7y-badge-text a-text-bold']/text()\"\n",
    "    if review.xpath(xpath_status) != []:\n",
    "        return review.xpath(xpath_status)[0]\n",
    "    else:\n",
    "        return \"none\"\n",
    "    \n",
    "def get_video_block(review):\n",
    "    xpath_video = \"div/div/span/div[starts-with(@id,'video-block')]\"\n",
    "    if review.xpath(xpath_video) != []:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reviews(page):\n",
    "    \n",
    "    review_dict = {\n",
    "    'asin': [],\n",
    "    'page': [],\n",
    "    'stars' : [],\n",
    "    'author': [],\n",
    "    'date': [],\n",
    "    'title':[],\n",
    "    'comment': [],\n",
    "    'verified': [],\n",
    "    'helpful': [],\n",
    "    'pics': [],\n",
    "    'video': [],\n",
    "    'comment_id': [],\n",
    "    'author_status':[]\n",
    "    }\n",
    "    \n",
    "    #set up the request\n",
    "    headers = {'User-Agent': ua.safari}\n",
    "    r = requests.get(page, headers=headers)\n",
    "    if r.status_code != 200:\n",
    "        print('status error',r.status_code,page)\n",
    "\n",
    "    #get test response from request\n",
    "    reviews_page = r.text\n",
    "\n",
    "    #parse the page\n",
    "    parser = html.fromstring(reviews_page)\n",
    "\n",
    "    # get the individual products\n",
    "    xpath_review = \"//div[@data-hook='review']\"\n",
    "    reviews = parser.xpath(xpath_review)\n",
    "\n",
    "    for review in reviews:\n",
    "        #add returned values to the list within the dictionary\n",
    "        review_dict['asin'].append(get_asin(review))\n",
    "        review_dict['page'].append(page)\n",
    "        review_dict['stars'].append(get_stars(review))\n",
    "        review_dict['title'].append(get_title(review))\n",
    "        review_dict['comment'].append(get_comment(review))\n",
    "        review_dict['author'].append(get_author(review))\n",
    "        review_dict['date'].append(get_date(review))\n",
    "        review_dict['comment_id'].append(get_review_id(review))\n",
    "        review_dict['verified'].append(get_verified(review))\n",
    "        review_dict['helpful'].append(get_helpful_count(review))\n",
    "        review_dict['author_status'].append(get_author_status(review))\n",
    "        review_dict['pics'].append(get_image_count(review))\n",
    "        review_dict['video'].append(get_video_block(review))\n",
    "\n",
    "    return review_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import product details to scrape reviews for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_play = pd.read_csv('amazon_playmobil.csv')\n",
    "df_play.drop(\"Unnamed: 0\",axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turn the product table into a list of urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_list = [['https://www.amazon.co.uk/'+str(df_play.titles[i].replace(\" \", \"-\")[0:42])+'/product-reviews/'+str(df_play.asins[i].replace(\n",
    "    '/', '-'))+'/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews&pageNumber='+str(p) for p in range(1, int(np.ceil(df_play.review_counts[i]/10))+1)] for i in range(len(df_play))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = list(itertools.chain(*big_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "function to generate the calls to the product review pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing.pool import ThreadPool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def async_get(urls):\n",
    "    ls_=[]  #list to store result\n",
    "    pool = ThreadPool(12) #generate a pool of 12 threads\n",
    "    results = pool.map_async(get_reviews, urls) #map the get_reviews function across all urls\n",
    "    results.wait() # blocking\n",
    "    ls_.append(results.get())  #store the result of the latest thread in the list\n",
    "    pool.close() #close the pool\n",
    "    pool.join() #close all threads\n",
    "    return ls_ #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE - you may want to periodically output reviews to a .csv so that if there is an error, not all is lost.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "t__ = async_get(urls)\n",
    "\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store Results in master dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_dict = {\n",
    "    'asin': [],\n",
    "    'page': [],\n",
    "    'stars' : [],\n",
    "    'author': [],\n",
    "    'date': [],\n",
    "    'title':[],\n",
    "    'comment': [],\n",
    "    'verified': [],\n",
    "    'helpful': [],\n",
    "    'pics': [],\n",
    "    'video': [],\n",
    "    'comment_id': [],\n",
    "    'author_status':[]\n",
    "    }\n",
    "\n",
    "df = pd.DataFrame(columns = review_dict.keys())\n",
    "\n",
    "for i in range(len(t__[0])):\n",
    "    df = df.append(pd.DataFrame(t__[0][i],columns=review_dict.keys()), ignore_index=True)\n",
    "    \n",
    "df.to_csv('playmobil_reviews.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
